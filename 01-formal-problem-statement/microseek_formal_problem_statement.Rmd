---
title: "MicroSeek Sample Classifier: Problem statement"
author: "Benjamin Cole"
date: "April 7, 2018"
output:
  html_document:
    toc: yes
  pdf_document:
    toc: yes
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction and client information
MicroSeek is a (fictional) microbiome sequencing and analysis company that contracts with clients in the healthcare, agricultural, and municipal sectors. Its mission is to provide its clients with accurate, comprehensive, and insightful microbial ecological information to drive decision making and discovery. The primary service that MicroSeek offers its clients is 16S ribosomal RNA (16S rRNA) sequence profiling of the microbial community in a sample (provided by the client). As all bacteria and archea species harbor distinct 16S rRNA molecules (identifiable at the sequence level), high-throughput sequencing of the entire population of these molecules makes it possible to identify an (often) characteristic abundance profile of organismal diversity in a given sample. Using these profiles, clients can make specific determinations about the origin of their samples, including:

* Abnormal gut microbiome content (could indicate gastrointestinal problems)
* Disease state of agricultural soil, pathogen counts on plant samples
* Clues as to the origin of an unknown sample from a crime scene

These samples can be in the form of unprocessed specimens (e.g. soil, fecal, or unknown samples), or in the form of pre-processed and isolated ribosomal RNA. Sequencing is performed after processing the sample (by extracting and amplifying rRNA molecules, if applicable, and then incorporating the processed sample into a sequencing library). The analysis team at MicroSeek then transforms the sequence data from these samples into tables containing the counts of each Operational Taxonomic Unit (OTU table; the number of each distinct 16S rRNA molecule observed in the sample). This OTU table is then further analyzed to look for insight into what features underly variation within the microbial composition (e.g. a spike in *Vibrionales* in food poisening patients), including any anomalies or sample quality issues. MicroSeek returns to the client the raw sequencing data, the processed OTU tables, and an analysis report.

The MicroSeek organizational structure consists of a top-level management team, which directs a production team and a research and development team. The production team, in addition to interfacing with clients, manages the sample management group (responsible for handling all physical samples and any technical processes that are associated with data generation) and the data analysis team, responsible for all data processing and analysis tasks. The data analysis team is split into focused groups that include subject matter experts associated with each client and sample type, and a sample QC team. The research and development team is responsible for advancing the technology profile of the company, and includes software developers, data scientists (overlapping with the production analysis group), and bench scientists.

# Problem
MicroSeek processes over 100,000 samples per week, and charges its clients ~$100 per sample (typically sent in ~100 batchs of 96 or 384). Data generation takes 24-48 hours per batch, however analysis efforts usually take up to 40 hours of an expert's time, depending on the complexity of the client's request (many clients request extensive analysis of their sample batches).

As samples from disparate sources (e.g. agricultural soils and human skin samples) are processed together to more efficiently utilize sequencing machine and reagent use, mistakes in sample processing are occasinally made, leading to mislabeling of materials, cross-contamination between samples, or sample degradation. While rare (< 5% of all batches), these issues take the analysis group a non-trivial amount of time to discover. Once the errors are discovered, MicroSeek can either resequence the samples on hand, or ask the clients for additional samples (if available); these measures are inexpensive, however the time required to realize the error can cost the company up to \$500 per batch in the data analysts' time and resources, sometimes totaling over $100k per year. Thus, correctly validating sample origin for all samples based on the information provided by the client, and the actual data generated for each sample, could potentially add confidence to the subsequent analysis performed.

In addition to sample QC, a sample validation/classification step can also help the company implement a new product type to its forensic science clients, yielding crude information about the most likely source of an unknown sample.

# Project goals and expectations
### Project summary
The MicroSeek management team has asked me (as part of its R&D group) to implement a sample classification tool that can predict the most likely physical source of a given sample, based on the microbiome composition profile (given by an OTU table), and how likely this source identification is. The model will then cross-reference this prediction with the sample source provided by the client, if applicable. 

### Project benefits
This method would flag potentially problematic samples (e.g soil samples that look like sea water samples) to be resequenced automatically prior to being examined by the data analysis team. It would also alert the analysis team to samples that might contain abnormalities, which would require additional resources to analyze, so they can better allocate their time. In addition to alleviating time spent on mislabeled specimens, the management team also hopes that an automated sample classifier will facilitate unknown sample identification efforts for its forensics clients.

### Success metrics
To significantly reduce the cost burden associated with sample mislabeling (by at least \$50,000 annually), the classifier should correctly identify at least 90% of all mislabeled specimens. In order to achieve this goal, the model itself should have an overal true positive rate (accuracy) of at least 90%. In addition, the model should report as few false positives as possible (under 5% as an upper threshold, to control for costs associated with looking into incorrectly flagged samples). If the model cannot attain the 90% threshold for success, the model can still be used as a less-important diagnostic for the internal data analysis group. The model may still hold value to identify extreme cases (e.g. if more than 50% of samples in a batch are mis-classified), however it will not carry the weight to significantly alter decisions on whether to resequence the samples, or to continue with data analysis on a more routine basis.

# Model deployment
In the project development phase, the development group will use only non-sensitive, publically available data to build the model, as currently provided by the [Earth Microbiome Project](http://www.earthmicrobiome.org/).

The classifer model will be deployed internally to the data analysis sample QC focus group as part of its automated QC pipeline. To facilitate this, the model itself will take the form of an R package, that can be easily integrated with other sample analysis tools, minimizing disruption to the normal workflow of the analysis group. The model will be used directly downstream of OTU table generation, but before any in-depth analysis is performed on the data. The model will take OTU tables either singly, or in batches, and return a table of problematic samples that can be flagged before further processing; the analysis QC team can then make a determination as to whether to proceed with in-depth data analysis or request additional sequencing to be performed.

# Project risk assessment
The deployment of this model should offer minimal risk to the company, and should make the data analysts' job easier, as they will spend less time dealing with aberrant data. Implementation will require some modification to the automated sample processing pipeline. These modifications should be minimal, however, since the sample QC pipeline already reports a table of QC metrics (e.g. number of OTUs discovered, alpha- and beta-diversity within each sample, and overall sequence quality), this model would only add three additional columns to that table (most likely sample origin, probability of most likely sample origin, and sample origin mis-match). The model implementation would also necessitate an additional QC checkpoint, manifested as a decision to proceed with data analysis given a potential sample mixup.

The deployment of this model might increase the number of samples flagged as needing resequencing, which could represent an additional cost burden (either in the sequencing itself, the loss of client confidence if faslely reported, or the QC analysts' time in chasing down false positives), however we hope to minimize these risks in choosing a model with a very low false positive rate.

# Project timeline
To develop this classification model, we will need to first restructure data from the public microbiome data repository, which could take up to 1-2 weeks, depending on how sparse the data are.

The model training and development will also likely take an additional 2-3 weeks, both to test various model types, and to evaluate each model's performance in two tests: performance over a test data set (from at least an internal and 1 additional data source), and performance on a simulated sample cross-contamination data set. At this point, we should have some preliminary data as to the feasibility of the project in meeting goals. 

Model deployment will take 1-2 weeks (to develop an R package and to further test its performance). This will include generation of an appropriate R package, to be distributed internally. We will also explore the creation of a Shiny R application to be used and distributed externally, if possible. This might be useful to deploy to clients to quickly classify their own data, if they have archival samples that they would like to re-analyze.

The model will be maintained in 4-month intervals, as new (and richer) data sets become available. This will involve re-evaluation of the model on new, simulated data: if the model continues to perform well, the same model will be re-trained on the new data, and then redeployed. If the model suffers more extreme degradation (e.g. power drops to below 75%), a new model will be developed and redeployed.